{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 manual process for analyzing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Import modules and create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n",
    "from pyspark.sql import SparkSession\n",
    "#create Spark session\n",
    "appName = \"Sentiment Analysis\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(appName) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Read data file into Spark dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------------+---------------------------------+\n",
      "|ItemID|Sentiment|SentimentSource|SentimentText                    |\n",
      "+------+---------+---------------+---------------------------------+\n",
      "|1038  |1        |Sentiment140   |that film is fantastic #brilliant|\n",
      "|1804  |1        |Sentiment140   |this music is really bad #myband |\n",
      "|1693  |0        |Sentiment140   |winter is terrible #thumbs-down  |\n",
      "+------+---------+---------------+---------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = spark.read.csv('dataset/tweets.csv', inferSchema=True, header=True)\n",
    "tweets.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Select the related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----+\n",
      "|SentimentText                    |label|\n",
      "+---------------------------------+-----+\n",
      "|that film is fantastic #brilliant|1    |\n",
      "|this music is really bad #myband |1    |\n",
      "|winter is terrible #thumbs-down  |0    |\n",
      "+---------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.select(\"SentimentText\", col('Sentiment').cast(\"Int\").alias(\"label\")).show(truncate=False,n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----+\n",
      "|SentimentText                    |label|\n",
      "+---------------------------------+-----+\n",
      "|that film is fantastic #brilliant|1    |\n",
      "|this music is really bad #myband |1    |\n",
      "|winter is terrible #thumbs-down  |0    |\n",
      "|this game is awful #nightmare    |0    |\n",
      "|I love jam #loveit               |1    |\n",
      "+---------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = tweets.select(\"SentimentText\", col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n",
    "data.show(truncate = False,n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data rows: 1364 ; Testing data rows: 568\n"
     ]
    }
   ],
   "source": [
    "#divide data, 70% for training, 30% for testing\n",
    "dividedData = data.randomSplit([0.7, 0.3]) \n",
    "trainingData = dividedData[0] \n",
    "testingData = dividedData[1] \n",
    "train_rows = trainingData.count()\n",
    "test_rows = testingData.count()\n",
    "print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Use a tokenizer to separate the SentimentText into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----+------------------------------+\n",
      "|SentimentText            |label|SentimentWords                |\n",
      "+-------------------------+-----+------------------------------+\n",
      "|I adore cheese #bestever |1    |[i, adore, cheese, #bestever] |\n",
      "|I adore cheese #favorite |1    |[i, adore, cheese, #favorite] |\n",
      "|I adore cheese #loveit   |1    |[i, adore, cheese, #loveit]   |\n",
      "|I adore cheese #thumbs-up|1    |[i, adore, cheese, #thumbs-up]|\n",
      "|I adore cheese #toptastic|1    |[i, adore, cheese, #toptastic]|\n",
      "+-------------------------+-----+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"SentimentText\", outputCol=\"SentimentWords\")\n",
    "tokenizedTrain = tokenizer.transform(trainingData)\n",
    "tokenizedTrain.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SentimentWords'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.getOutputCol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Remove stop words (unimportant words that will not be used as features).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----+------------------------------+---------------------------+\n",
      "|SentimentText            |label|SentimentWords                |MeaningfulWords            |\n",
      "+-------------------------+-----+------------------------------+---------------------------+\n",
      "|I adore cheese #bestever |1    |[i, adore, cheese, #bestever] |[adore, cheese, #bestever] |\n",
      "|I adore cheese #favorite |1    |[i, adore, cheese, #favorite] |[adore, cheese, #favorite] |\n",
      "|I adore cheese #loveit   |1    |[i, adore, cheese, #loveit]   |[adore, cheese, #loveit]   |\n",
      "|I adore cheese #thumbs-up|1    |[i, adore, cheese, #thumbs-up]|[adore, cheese, #thumbs-up]|\n",
      "|I adore cheese #toptastic|1    |[i, adore, cheese, #toptastic]|[adore, cheese, #toptastic]|\n",
      "+-------------------------+-----+------------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                       outputCol=\"MeaningfulWords\")\n",
    "SwRemovedTrain = swr.transform(tokenizedTrain)\n",
    "SwRemovedTrain.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Convert the words that will be used as features into numerical values. \n",
    "-  In Spark 2.2.1, this is implemented using the HashingTF function \n",
    "-  Austin Appleby's MurmurHash 3 algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SentimentText', 'label', 'SentimentWords', 'MeaningfulWords', 'features']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "hashTF.transform(SwRemovedTrain).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------+-------------------------------------------+\n",
      "|label|MeaningfulWords           |features                                   |\n",
      "+-----+--------------------------+-------------------------------------------+\n",
      "|1    |[adore, cheese, #bestever]|(262144,[1689,91011,100089],[1.0,1.0,1.0]) |\n",
      "|1    |[adore, cheese, #favorite]|(262144,[1689,100089,108624],[1.0,1.0,1.0])|\n",
      "|1    |[adore, cheese, #loveit]  |(262144,[1689,100089,254974],[1.0,1.0,1.0])|\n",
      "+-----+--------------------------+-------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "numericTrainData = hashTF.transform(SwRemovedTrain).select(\n",
    "            'label', 'MeaningfulWords', 'features')\n",
    "numericTrainData.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Use the training data to train our classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", \n",
    "                        maxIter=10, regParam=0.01)\n",
    "svc = LinearSVC(featuresCol='features',labelCol='label')\n",
    "\n",
    "model = lr.fit(numericTrainData)\n",
    "model1 = svc.fit(numericTrainData) \n",
    "print (\"Training is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Prepare the testing data for use in evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------+--------------------------------------------------------+\n",
      "|Label|MeaningfulWords                   |features                                                |\n",
      "+-----+----------------------------------+--------------------------------------------------------+\n",
      "|1    |[adore, cheese, #brilliant]       |(262144,[1689,45361,100089],[1.0,1.0,1.0])              |\n",
      "|1    |[adore, classical, music, #loveit]|(262144,[100089,102383,131250,254974],[1.0,1.0,1.0,1.0])|\n",
      "+-----+----------------------------------+--------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizedTest = tokenizer.transform(testingData)\n",
    "SwRemovedTest = swr.transform(tokenizedTest)\n",
    "numericTest = hashTF.transform(SwRemovedTest).select(\n",
    "    'Label', 'MeaningfulWords', 'features')\n",
    "numericTest.show(truncate=False, n=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Predict testing data and calculate the accuracy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = model.transform(numericTest)\n",
    "prediction1 = model1.transform(numericTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionFinal = prediction.select(\n",
    "    \"MeaningfulWords\", \"prediction\", \"Label\")\n",
    "predictionFinal1 = prediction1.select(\n",
    "    \"MeaningfulWords\", \"prediction\", \"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+\n",
      "|     MeaningfulWords|prediction|Label|\n",
      "+--------------------+----------+-----+\n",
      "|[adore, cheese, #...|       1.0|    1|\n",
      "|[adore, classical...|       1.0|    1|\n",
      "|[adore, coffee, #...|       1.0|    1|\n",
      "+--------------------+----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/14 07:20:01 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n"
     ]
    }
   ],
   "source": [
    "predictionFinal1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/14 07:20:02 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n"
     ]
    }
   ],
   "source": [
    "correctPrediction = predictionFinal.filter(\n",
    "    predictionFinal['prediction'] == predictionFinal['Label']).count()\n",
    "correctPrediction1 = predictionFinal1.filter(\n",
    "    predictionFinal1['prediction'] == predictionFinal1['Label']).count()\n",
    "totalData = predictionFinal.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 562 , total data: 568 , accuracy: 0.9894366197183099\n",
      "correct prediction1: 562 , total data: 568 , accuracy: 0.9894366197183099\n"
     ]
    }
   ],
   "source": [
    "print(\"correct prediction:\", correctPrediction, \", total data:\", totalData, \n",
    "      \", accuracy:\", correctPrediction/totalData)\n",
    "print(\"correct prediction1:\", correctPrediction1, \", total data:\", totalData, \n",
    "      \", accuracy:\", correctPrediction1/totalData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Create pipeline to automate all stages of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = spark.read.csv('dataset/tweets.csv', inferSchema=True, header=True)\n",
    "data = tweets.select(\"SentimentText\", col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n",
    "\n",
    "dividedData = data.randomSplit([0.7, 0.3]) \n",
    "trainingData = dividedData[0] \n",
    "testingData = dividedData[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create a tokenizer to split the SentimentText into individual words\n",
    "tokenizer = Tokenizer(inputCol='SentimentText', outputCol='words')\n",
    "\n",
    "# Create a stop words remover\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='filtered_words')\n",
    "\n",
    "# Create a CountVectorizer or HashingTF to convert the words into numerical values\n",
    "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol='features')\n",
    "\n",
    "# Use a feature selector to select the relevant fields\n",
    "selector = VectorAssembler(inputCols=['features'], outputCol='selected_features')\n",
    "\n",
    "# Create a classification model\n",
    "classifier = LogisticRegression(featuresCol=selector.getOutputCol(),labelCol='label' )\n",
    "\n",
    "# Put everything in a pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, selector, classifier])\n",
    "\n",
    "# Fit the model to the data\n",
    "model = pipeline.fit(trainingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = testingData.select(\"SentimentText\")\n",
    "df.write.format(\"csv\").save(\"../Datagenerator/tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|       SentimentText|               words|      filtered_words|            features|   selected_features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|I adore classical...|[i, adore, classi...|[adore, classical...|(50,[0,15,18,36],...|(50,[0,15,18,36],...|[-29.047959248792...|[2.42455247297138...|       1.0|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(tsts).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|       SentimentText|label|               words|      filtered_words|            features|   selected_features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|I adore classical...|    1|[i, adore, classi...|[adore, classical...|(50,[0,15,18,36],...|(50,[0,15,18,36],...|[-29.047959248792...|[2.42455247297138...|       1.0|\n",
      "|I adore coffee #b...|    1|[i, adore, coffee...|[adore, coffee, #...|(50,[15,16,44],[1...|(50,[15,16,44],[1...|[-4.6329350146634...|[0.00963248353624...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions1 = model.transform(testingData)\n",
    "predictions1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+\n",
      "|      filtered_words|prediction|Label|\n",
      "+--------------------+----------+-----+\n",
      "|[adore, classical...|       1.0|    1|\n",
      "|[adore, coffee, #...|       1.0|    1|\n",
      "+--------------------+----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionFinal =  predictions1.select(\n",
    "    \"filtered_words\", \"prediction\", \"Label\")\n",
    "# accuracy\n",
    "predictionFinal.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9897435897435898\n"
     ]
    }
   ],
   "source": [
    "correctPrediction = predictionFinal.filter(\n",
    "    predictionFinal['prediction'] == predictionFinal['Label']).count()\n",
    "totalData = predictionFinal.count()\n",
    "print( f\"Accuracy {correctPrediction/totalData}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"SentimentAnalysis\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "7150a245f6f7dc8432796ce1350d5d6c720c519331591bb47fe5a08c99518d34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
